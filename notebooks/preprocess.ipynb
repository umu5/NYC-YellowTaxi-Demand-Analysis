{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "import datetime\n",
    "from pyspark.sql.functions import expr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = spark.read.parquet('../data/weather/weather_data.parquet')\n",
    "sdf1.printSchema()\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "# check if there are any null values\n",
    "missing_values = sdf1.agg(*[F.sum(F.when(F.col(c).isNull(), 1).otherwise(0))\n",
    "                          .alias(c) for c in sdf1.columns])\n",
    "missing_values.show()\n",
    "# check if there are any null values\n",
    "\n",
    "# cast date column to date type\n",
    "sdf1 = sdf1.withColumn(\"DATE\", col(\"DATE\").cast(\"date\"))\n",
    "\n",
    "# also drop the PGTM column as it is not useful\n",
    "sdf1 = sdf1.drop('PGTM')\n",
    "\n",
    "# also as per industry practice convert to consistent casing\n",
    "consistent_casing = [F.col(col_name).alias(col_name.lower())\n",
    "                    for col_name in sdf1.columns]\n",
    "sdf1 = sdf1.select(*consistent_casing)\n",
    "sdf1.printSchema()\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "# create directory for raw weather data and store it there\n",
    "if not os.path.exists('../data/weather_curated/'):\n",
    "    os.mkdir('../data/weather_curated/')\n",
    "\n",
    "# now write this file and if any other present overwrite it\n",
    "sdf1.write.parquet('../data/weather_curated/weather_data.parquet',\\\n",
    "                 mode ='overwrite')\n",
    "\n",
    "# there is no gap between weather raw and curated layer beacuse donot require\n",
    "# much transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets begin some preprocessing on yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('../data/tlc/')\n",
    "sdf_jan = spark.read.parquet('../data/tlc/2022-01.parquet')\n",
    "sdf_jan.printSchema()\n",
    "sdf_feb_23 = spark.read.parquet('../data/tlc/2023-02.parquet')\n",
    "sdf_feb_23.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_schema(year, start, end, temp_schema):\n",
    "    \"\"\" This function takes in the year, start and end month and the schema\n",
    "    and casts the columns to the data type and saves it to the data/raw/\n",
    "    directory. `year` is a string, `start` and `end` are integers months and\n",
    "    `temp_schema` is a spark schema.\n",
    "    This function is specific for the TLC data to store data to raw layer.\n",
    "    \"\"\"\n",
    "    for month in range(start, end+1):\n",
    "        input_path = f'../data/tlc/{year}-{str(month).zfill(2)}.parquet'\n",
    "        output_path = f'../data/raw/{year}-{str(month).zfill(2)}.parquet'\n",
    "        sdf_malformed = spark.read.parquet(input_path)\n",
    "        sdf_malformed = sdf_malformed \\\n",
    "                        .select([F.col(c).cast(temp_schema[i].dataType)\\\n",
    "                        for i, c in enumerate(sdf_malformed.columns)])\n",
    "        sdf_malformed.coalesce(1).write.mode('overwrite').parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there is any non-whole number passenger_count. So it can be removed\n",
    "# before any proper data conversion can be made. Lets cast all data to \n",
    "# jan 2022 schema which contains passenger_count as \"double\"\n",
    "tem_schema = sdf_jan.schema\n",
    "tem_schema\n",
    "cast_to_schema(\"2022\", 1, 12, tem_schema)\n",
    "cast_to_schema(\"2023\", 1, 5, tem_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there is any non-whole number passenger_count\n",
    "sdf = spark.read.parquet('../data/raw/*')\n",
    "non_whole_count = sdf.filter(col(\"passenger_count\") % 1 != 0).count()\n",
    "print(f\"Number of rows with non-whole number passenger : {non_whole_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well we are now sure there wasn't any non-whole number passenger_count\n",
    "# so lets try to cast all datatype to 2023 february schema (proper schema)\n",
    "sdf_feb_23.printSchema()\n",
    "\n",
    "# also ensuring we have consistent casing\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower())\\\n",
    "                        for col_name in sdf_feb_23.columns]\n",
    "sdf_feb_23 = sdf_feb_23.select(*consistent_col_casing)\n",
    "sdf_schema = sdf_feb_23.schema\n",
    "sdf_schema\n",
    "\n",
    "cast_to_schema(2022, 1, 12, sdf_schema)\n",
    "cast_to_schema(2023, 1, 5, sdf_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw data to verify\n",
    "sdf = spark.read.parquet('../data/raw/*')\n",
    "sdf.printSchema()\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done with raw data &#128512;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to data dictionary store_and_fwd_flag represents boolean condition\n",
    "# but currently have N and Y to represent No and Yes respectively\n",
    "sdf = sdf.withColumn('store_and_fwd_flag', \n",
    "    (F.col('store_and_fwd_flag') == 'Y').cast('boolean'))\n",
    "sdf.printSchema()\n",
    "sdf.show(1, vertical=True, truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the datashape before doing any further preprocessing\n",
    "tot_rows = sdf.count()\n",
    "tot_cols = len(sdf.columns)\n",
    "print(\"Total rows: \", tot_rows, \"Total columns: \", tot_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see which columns have missing values\n",
    "missing_values = sdf.agg(*[F.sum(F.when(F.col(c).isNull(), 1)\\\n",
    "                        .otherwise(0)).alias(c) for c in sdf.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well there is immense amount of missing passenger data. \n",
    "# (imputation doesn't makes sense here) so let drop those (1796968) rows\n",
    "missing_passenger = (missing_values.collect()[0]['passenger_count'] \n",
    "                    / tot_rows * 100)\n",
    "print(f'percentage of missing passenger_count data: {missing_passenger}%')\n",
    "\n",
    "sdf_clean = sdf.filter(col(\"passenger_count\").isNotNull())\n",
    "missing_values2 = sdf_clean.agg(*[F.sum(F.when(F.col(c).isNull(), 1)\\\n",
    "                            .otherwise(0)).alias(c) for c in sdf_clean.columns])\n",
    "missing_values2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`after removing misssing passenger_count no other values seems to be missing` \\\n",
    "`How nice is this dataset? No missing values at all.`\n",
    "## lets do some outlier detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new temporary column to record trip distance so can remove those\n",
    "# trips whcih span more than 5 hours\n",
    "sdf_with_difference = sdf_clean.withColumn(\"time_difference\", \n",
    "                                          col(\"tpep_dropoff_datetime\")\n",
    "                                          - col(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# Count rows where the time_difference is more than 5 hours and remove them\n",
    "count_greater_than_5_hours = sdf_with_difference \\\n",
    "                             .filter(expr(\"time_difference \"\n",
    "                             \"> interval 5 hours\")).count()\n",
    "\n",
    "print(f\"Number of rows with time_difference greater than 5 hours: \"\n",
    "      f\"{count_greater_than_5_hours}, percentage: \" \n",
    "      f\"{count_greater_than_5_hours/sdf_with_difference.count() * 100}\")\n",
    "                                         \n",
    "sdf_clean2 = sdf_with_difference \\\n",
    "            .filter(expr(\"time_difference <= interval 5 hours\"))\n",
    "# no further need of time_difference column\n",
    "sdf_clean2 = sdf_clean2.drop(\"time_difference\")\n",
    "\n",
    "# also remove those rows that records drop off time before pick up time\n",
    "invalid_dropoff = sdf_clean2.filter(expr(\"tpep_dropoff_datetime\"\n",
    "                                    \" <= tpep_pickup_datetime\")).count()\n",
    "\n",
    "print(f\"Number of rows with invalid drop off time: {invalid_dropoff},\" \n",
    "      f\"percentage: {invalid_dropoff/sdf_clean2.count() * 100}\")\n",
    "\n",
    "sdf_clean2 = sdf_clean2.filter(expr(\"tpep_dropoff_datetime\"\n",
    "                                    \" > tpep_pickup_datetime\"))\n",
    "print(f'remaining rows {sdf_clean2.count()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reomove Trips with a pick-up/drop-off location ID out of the range 1-263\n",
    "sdf_clean3 = sdf_clean2.filter(expr(\"PULocationID >= 1 \"\n",
    "                                    \"AND PULocationID <= 263 \"\n",
    "                                    \"AND DOLocationID >= 1 \"\n",
    "                                    \"AND DOLocationID <= 263\"))\n",
    "\n",
    "invalid_id = sdf_clean2.count() - sdf_clean3.count()\n",
    "print(f\"Number of invalid location IDs: {invalid_id}\",\n",
    "      f\"percentage: {invalid_id/sdf_clean2.count()*100}%\")\n",
    "print(sdf_clean3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with passenger count 0 or less\n",
    "sdf_clean4 = sdf_clean3.filter(expr(\"passenger_count > 0\"))\n",
    "data_count = sdf_clean3.count()\n",
    "invalid_passenger_count = data_count - sdf_clean4.count()\n",
    "print(f\"invalid passenger count: {invalid_passenger_count}\"\n",
    "      f\" percentage: {invalid_passenger_count/data_count*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with trip_distance 0 or less\n",
    "sdf_clean5 = sdf_clean4.filter(expr(\"trip_distance > 0\"))\n",
    "data_count = sdf_clean4.count()\n",
    "invalid_trip_dis = data_count - sdf_clean5.count()\n",
    "print(f\"Number of invalid trip_distance records: {invalid_trip_dis}\"\n",
    "      f\" percentage: {invalid_trip_dis/data_count*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if other outliers present according to data dictionary\n",
    "print(sdf_clean5.filter(expr('VendorID > 2 AND vendorID < 1')).count())\n",
    "print(sdf_clean5.filter(expr('RateCodeID > 6 AND RateCodeID < 1')).count())\n",
    "print(sdf_clean5.filter(expr('payment_type > 6 AND payment_type < 1')).count())\n",
    "print(sdf_clean5.filter(expr(\"improvement_surcharge < 0 OR fare_amount < 0\"\n",
    "                            \" OR extra < 0 OR mta_tax < 0 or tip_amount < 0\"\n",
    "                            \" OR total_amount < 0 OR congestion_surcharge < 0\"\n",
    "                            \" OR airport_fee < 0 OR tip_amount < 0\"\n",
    "                            \" OR tolls_amount < 0\")).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we have some outliers in above cell lets remove those\n",
    "sdf_clean6 = sdf_clean5.filter(expr(\" improvement_surcharge >= 0\"\n",
    "                                    \" AND fare_amount >= 0 AND extra >= 0\"\n",
    "                                    \" AND mta_tax >= 0 AND tip_amount >= 0\"\n",
    "                                    \" AND total_amount >= 0\"\n",
    "                                    \" AND congestion_surcharge >= 0\"\n",
    "                                    \" AND airport_fee >= 0 AND tip_amount >= 0\"\n",
    "                                    \" AND tolls_amount >= 0\"))\n",
    "prev = sdf_clean5.count()\n",
    "removed = prev - sdf_clean6.count()\n",
    "print(f\"rows removed {removed} \"\n",
    "      f\"percentage: {removed/prev*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHEWWW! Done with all yellow taxi data preprocessing &#128512;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now store data in curated layer\n",
    "sdf_clean6.write.parquet('../data/curated/yellow_taxi.parquet',\\\n",
    "                        mode ='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
